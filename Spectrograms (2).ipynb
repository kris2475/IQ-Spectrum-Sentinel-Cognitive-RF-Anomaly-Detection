{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4AAYbhmQHri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c2ef538"
      },
      "source": [
        "## Starting Over: Building, Training, and Evaluating a Conv2D Autoencoder\n",
        "\n",
        "Starting fresh to go through the entire process step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d968d3d3"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "Mount your Google Drive to access files stored there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817433bf",
        "outputId": "443e3267-522a-4ac0-8c1d-9d0d74d42082"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea0f4b5e"
      },
      "source": [
        "### Define Paths for Training and Testing Data\n",
        "\n",
        "Specify the paths for the zip files containing the training and testing data in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e714a6b2",
        "outputId": "5706e7f9-45ab-4965-b9d1-f4fdece1c26a"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory for zip files\n",
        "zip_base_dir = '/content/drive/My Drive/ZIP/'\n",
        "\n",
        "# Define the path for the test data zip file (corrected filename)\n",
        "test_zip_file = os.path.join(zip_base_dir, 'spectrograms_anomolous.zip')\n",
        "\n",
        "# Define the list of training data zip files (excluding the test file)\n",
        "all_zip_files = os.listdir(zip_base_dir)\n",
        "training_zip_files = [os.path.join(zip_base_dir, f) for f in all_zip_files if f.endswith('.zip') and f != 'spectrograms_anomolous.zip']\n",
        "\n",
        "print(f\"Test data zip file: {test_zip_file}\")\n",
        "print(\"Training data zip files:\")\n",
        "for f in training_zip_files:\n",
        "    print(f)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data zip file: /content/drive/My Drive/ZIP/spectrograms_anomolous.zip\n",
            "Training data zip files:\n",
            "/content/drive/My Drive/ZIP/spectrograms_full_extended.zip\n",
            "/content/drive/My Drive/ZIP/spectrograms_full_ext2.zip\n",
            "/content/drive/My Drive/ZIP/spectrograms_full_ext1.zip\n",
            "/content/drive/My Drive/ZIP/spectrograms_collapsed.zip\n",
            "/content/drive/My Drive/ZIP/spectrograms_45_deg.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b93f58e0"
      },
      "source": [
        "### Create Extraction Directories\n",
        "\n",
        "Create separate directories to store the extracted files for training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75f89871",
        "outputId": "81568bac-ec25-406a-e588-138903ba1e4e"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define directories for extracting training and testing data\n",
        "train_extract_dir = '/tmp/extracted_training_data'\n",
        "test_extract_dir = '/tmp/extracted_testing_data'\n",
        "\n",
        "# Clean up the directories before extraction to ensure a clean slate\n",
        "print(f\"Cleaning up extraction directories: {train_extract_dir} and {test_extract_dir}\")\n",
        "if os.path.exists(train_extract_dir):\n",
        "    shutil.rmtree(train_extract_dir)\n",
        "os.makedirs(train_extract_dir, exist_ok=True)\n",
        "\n",
        "if os.path.exists(test_extract_dir):\n",
        "    shutil.rmtree(test_extract_dir)\n",
        "os.makedirs(test_extract_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Re-created training data extraction directory: {train_extract_dir}\")\n",
        "print(f\"Re-created testing data extraction directory: {test_extract_dir}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up extraction directories: /tmp/extracted_training_data and /tmp/extracted_testing_data\n",
            "Re-created training data extraction directory: /tmp/extracted_training_data\n",
            "Re-created testing data extraction directory: /tmp/extracted_testing_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0f45e73"
      },
      "source": [
        "### Extract Training Data\n",
        "\n",
        "Extract the contents of the zip files designated for training into the training data directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a60be89b",
        "outputId": "ed8fc34d-d3c1-4d19-a22d-326dde81be04"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Use the training_zip_files list defined earlier\n",
        "# Use the train_extract_dir defined earlier\n",
        "\n",
        "# Clean up any previous anomalous data extraction in the training directory\n",
        "anomalous_dir_in_train = os.path.join(train_extract_dir, 'spectrograms_anomolous')\n",
        "if os.path.exists(anomalous_dir_in_train):\n",
        "    print(f\"Removing anomalous data from training directory: {anomalous_dir_in_train}\")\n",
        "    shutil.rmtree(anomalous_dir_in_train)\n",
        "\n",
        "print(\"Extracting training data...\")\n",
        "for zip_file in training_zip_files:\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            print(f\"Extracting {zip_file} to {train_extract_dir}...\")\n",
        "            zip_ref.extractall(train_extract_dir)\n",
        "            print(\"Extraction complete.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Training zip file not found at {zip_file}. Skipping extraction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {zip_file}: {e}\")\n",
        "\n",
        "print(\"\\nTraining data extraction complete.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting training data...\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_full_extended.zip to /tmp/extracted_training_data...\n",
            "Extraction complete.\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_full_ext2.zip to /tmp/extracted_training_data...\n",
            "Extraction complete.\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_full_ext1.zip to /tmp/extracted_training_data...\n",
            "Extraction complete.\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_collapsed.zip to /tmp/extracted_training_data...\n",
            "Extraction complete.\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_45_deg.zip to /tmp/extracted_training_data...\n",
            "Extraction complete.\n",
            "\n",
            "Training data extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a311b6d"
      },
      "source": [
        "### Extract Testing Data\n",
        "\n",
        "Extract the contents of `spectrograms_anomalous.zip` into the testing data directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfea7390",
        "outputId": "7b104875-fbd4-4734-c2ff-f1c0073f44c7"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Use the test_zip_file variable defined earlier\n",
        "# Use the test_extract_dir variable defined earlier\n",
        "\n",
        "print(\"Extracting testing data...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(test_zip_file, 'r') as zip_ref:\n",
        "        print(f\"Extracting {test_zip_file} to {test_extract_dir}...\")\n",
        "        zip_ref.extractall(test_extract_dir)\n",
        "        print(\"Extraction complete.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Test zip file not found at {test_zip_file}. Skipping extraction.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting {test_zip_file}: {e}\")\n",
        "\n",
        "print(\"\\nTesting data extraction complete.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting testing data...\n",
            "Extracting /content/drive/My Drive/ZIP/spectrograms_anomolous.zip to /tmp/extracted_testing_data...\n",
            "Extraction complete.\n",
            "\n",
            "Testing data extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45348fd1"
      },
      "source": [
        "### Verify Extracted Files\n",
        "\n",
        "List the contents of both the training and testing directories to confirm the files have been extracted correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dc5d5f4",
        "outputId": "13451304-51db-4934-8074-c27e3c909b69"
      },
      "source": [
        "import os\n",
        "\n",
        "print(f\"Contents of training directory ({train_extract_dir}):\")\n",
        "if os.path.isdir(train_extract_dir):\n",
        "    training_contents = os.listdir(train_extract_dir)\n",
        "    if training_contents:\n",
        "        for item in training_contents:\n",
        "            print(f\"- {item}\")\n",
        "        print(\"\\nChecking subdirectories for .npy files:\")\n",
        "        found_npy_train = False\n",
        "        for root, dirs, files in os.walk(train_extract_dir):\n",
        "            npy_files = [f for f in files if f.endswith('.npy')]\n",
        "            if npy_files:\n",
        "                print(f\"  Found {len(npy_files)} .npy files in {root} (showing first 5):\")\n",
        "                for i, fname in enumerate(npy_files[:5]):\n",
        "                    print(f\"    {fname}\")\n",
        "                found_npy_train = True\n",
        "        if not found_npy_train:\n",
        "            print(\"  No .npy files found in any subdirectories of the training extraction path.\")\n",
        "    else:\n",
        "        print(\"Training directory is empty.\")\n",
        "else:\n",
        "    print(f\"Error: Training directory not found at {train_extract_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "print(f\"Contents of testing directory ({test_extract_dir}):\")\n",
        "if os.path.isdir(test_extract_dir):\n",
        "    testing_contents = os.listdir(test_extract_dir)\n",
        "    if testing_contents:\n",
        "        for item in testing_contents:\n",
        "            print(f\"- {item}\")\n",
        "        print(\"\\nChecking subdirectories for .npy files:\")\n",
        "        found_npy_test = False\n",
        "        for root, dirs, files in os.walk(test_extract_dir):\n",
        "            npy_files = [f for f in files if f.endswith('.npy')]\n",
        "            if npy_files:\n",
        "                print(f\"  Found {len(npy_files)} .npy files in {root} (showing first 5):\")\n",
        "                for i, fname in enumerate(npy_files[:5]):\n",
        "                    print(f\"    {fname}\")\n",
        "                found_npy_test = True\n",
        "        if not found_npy_test:\n",
        "             print(\"  No .npy files found in any subdirectories of the testing extraction path.\")\n",
        "    else:\n",
        "        print(\"Testing directory is empty.\")\n",
        "else:\n",
        "    print(f\"Error: Testing directory not found at {test_extract_dir}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of training directory (/tmp/extracted_training_data):\n",
            "- spectrograms_full_ext2\n",
            "- spectrograms_collapsed\n",
            "- spectrograms_full_extended\n",
            "- spectrograms_45_deg\n",
            "- spectrograms_full_ext1\n",
            "\n",
            "Checking subdirectories for .npy files:\n",
            "  Found 726 .npy files in /tmp/extracted_training_data/spectrograms_full_ext2 (showing first 5):\n",
            "    spec_643MHz_20251025_080800.npy\n",
            "    spec_259MHz_20251025_080730.npy\n",
            "    spec_809MHz_20251025_080813.npy\n",
            "    spec_413MHz_20251025_080742.npy\n",
            "    spec_62MHz_20251025_080703.npy\n",
            "  Found 726 .npy files in /tmp/extracted_training_data/spectrograms_collapsed (showing first 5):\n",
            "    spec_742MHz_20251025_075521.npy\n",
            "    spec_1439MHz_20251025_075616.npy\n",
            "    spec_1141MHz_20251025_075552.npy\n",
            "    spec_965MHz_20251025_075538.npy\n",
            "    spec_1275MHz_20251025_075603.npy\n",
            "  Found 726 .npy files in /tmp/extracted_training_data/spectrograms_full_extended (showing first 5):\n",
            "    spec_872MHz_20251025_074950.npy\n",
            "    spec_1705MHz_20251025_075056.npy\n",
            "    spec_1174MHz_20251025_075014.npy\n",
            "    spec_795MHz_20251025_074944.npy\n",
            "    spec_1025MHz_20251025_075002.npy\n",
            "  Found 726 .npy files in /tmp/extracted_training_data/spectrograms_45_deg (showing first 5):\n",
            "    spec_715MHz_20251025_075850.npy\n",
            "    spec_1374MHz_20251025_075948.npy\n",
            "    spec_281MHz_20251025_075814.npy\n",
            "    spec_1636MHz_20251025_080010.npy\n",
            "    spec_1679MHz_20251025_080014.npy\n",
            "  Found 726 .npy files in /tmp/extracted_training_data/spectrograms_full_ext1 (showing first 5):\n",
            "    spec_944MHz_20251025_080332.npy\n",
            "    spec_1650MHz_20251025_080433.npy\n",
            "    spec_177MHz_20251025_080225.npy\n",
            "    spec_1477MHz_20251025_080418.npy\n",
            "    spec_809MHz_20251025_080321.npy\n",
            "\n",
            "==============================\n",
            "\n",
            "Contents of testing directory (/tmp/extracted_testing_data):\n",
            "- spectrograms_anomolous\n",
            "\n",
            "Checking subdirectories for .npy files:\n",
            "  Found 726 .npy files in /tmp/extracted_testing_data/spectrograms_anomolous (showing first 5):\n",
            "    spec_1117MHz_20251025_082332.npy\n",
            "    spec_715MHz_20251025_082258.npy\n",
            "    spec_1741MHz_20251025_082420.npy\n",
            "    spec_182MHz_20251025_082202.npy\n",
            "    spec_1129MHz_20251025_082333.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4af737bb"
      },
      "source": [
        "### Implement Data Generator for Conv2D\n",
        "\n",
        "Implement a custom data generator to load 2D spectrogram `.npy` files in batches to manage memory usage for the Conv2D model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c498b0f",
        "outputId": "49032b6a-645b-42e3-bfaf-f8af88708625"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class SpectrogramDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Data Generator for loading 2D spectrograms from .npy files in batches for Conv2D input.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_paths, batch_size=32, shuffle=True, input_shape=(1024, 292, 1)): # Expected Conv2D input shape\n",
        "        self.file_paths = file_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        # Input shape should now include the channel dimension for Conv2D\n",
        "        self.input_shape = input_shape\n",
        "        self.indexes = np.arange(len(self.file_paths))\n",
        "        if self.shuffle:\n",
        "            self.on_epoch_end()\n",
        "        # Determine the expected 2D shape without the channel for loading\n",
        "        self._loading_shape = input_shape[:2]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        # Ensure we don't have empty batches at the end if drop_last is not used\n",
        "        return int(np.floor(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # List of file paths for the batch\n",
        "        batch_file_paths = [self.file_paths[k] for k in indexes]\n",
        "\n",
        "        # Load and collect data for the batch\n",
        "        batch_data = []\n",
        "        for file_path in batch_file_paths:\n",
        "            try:\n",
        "                data = np.load(file_path, mmap_mode='r')\n",
        "                # Ensure data matches the expected loading shape (without channel)\n",
        "                if data.shape == self._loading_shape:\n",
        "                    # Add channel dimension (grayscale)\n",
        "                    batch_data.append(np.expand_dims(data, axis=-1))\n",
        "                else:\n",
        "                    print(f\"Warning: Spectrogram shape mismatch for {file_path}. Expected {self._loading_shape}, got {data.shape}. Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading file {file_path} in generator: {e}\")\n",
        "\n",
        "        if not batch_data:\n",
        "             # If no data was loaded for this batch, return empty arrays\n",
        "             # This can happen if all files in the batch had errors or shape mismatches\n",
        "             # Returning empty arrays with the expected output shape\n",
        "             return np.empty((0, *self.input_shape)), np.empty((0, *self.input_shape))\n",
        "\n",
        "\n",
        "        # Concatenate data from all files in the batch\n",
        "        # Assuming each file contains one 2D spectrogram\n",
        "        try:\n",
        "             batch_data = np.stack(batch_data, axis=0) # Stack along the new batch dimension\n",
        "        except ValueError as e:\n",
        "             print(f\"Error stacking data for batch index {index}: {e}\")\n",
        "             # This might happen if loaded data chunks have inconsistent shapes\n",
        "             # Returning empty arrays on error\n",
        "             return np.empty((0, *self.input_shape)), np.empty((0, *self.input_shape))\n",
        "\n",
        "\n",
        "        # Assuming the autoencoder input and output are the same\n",
        "        return batch_data, batch_data\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "print(\"SpectrogramDataGenerator class defined for Conv2D.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectrogramDataGenerator class defined for Conv2D.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42c36a21"
      },
      "source": [
        "### Determine Spectrogram Shape and Create Generators\n",
        "\n",
        "Determine the consistent shape of the spectrograms and create data generator instances for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b593e6b2",
        "outputId": "f7205665-b5b4-4e9a-bea0-99ad012670d7"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Determine spectrogram shape and define consistent shape for the generator\n",
        "print(\"Determining spectrogram shape...\")\n",
        "train_extract_base_dir = '/tmp/extracted_training_data'\n",
        "all_training_files = []\n",
        "for dirpath, dirnames, filenames in os.walk(train_extract_base_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.npy'):\n",
        "            all_training_files.append(os.path.join(dirpath, filename))\n",
        "\n",
        "spectrogram_shape = None\n",
        "if all_training_files:\n",
        "    try:\n",
        "        # Load an example to get the shape\n",
        "        example_data = np.load(all_training_files[0], mmap_mode='r')\n",
        "        spectrogram_shape = example_data.shape\n",
        "        print(f\"Inferred spectrogram shape: {spectrogram_shape}\")\n",
        "        # Assuming all spectrograms have the same shape for simplicity.\n",
        "        # If shapes vary, padding or resizing would be needed.\n",
        "        consistent_shape = spectrogram_shape\n",
        "        print(f\"Using consistent shape: {consistent_shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error inferring spectrogram shape: {e}\")\n",
        "        consistent_shape = (1024, 292) # Fallback to a known shape if inference fails\n",
        "        print(f\"Using fallback consistent shape: {consistent_shape}\")\n",
        "else:\n",
        "    consistent_shape = (1024, 292) # Fallback if no training files found\n",
        "    print(f\"No training files found to infer shape. Using fallback consistent shape: {consistent_shape}\")\n",
        "\n",
        "# Create Data Generator Instances with updated input shape and correct paths\n",
        "print(\"\\nCreating data generator instances with 2D input shape and correct paths...\")\n",
        "test_extract_base_dir = '/tmp/extracted_testing_data' # Base dir for testing\n",
        "\n",
        "all_testing_files = []\n",
        "for dirpath, dirnames, filenames in os.walk(test_extract_base_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.npy'):\n",
        "            all_testing_files.append(os.path.join(dirpath, filename))\n",
        "\n",
        "\n",
        "if consistent_shape is not None:\n",
        "    # Input shape for the generator and model, including the channel dimension\n",
        "    generator_input_shape = (*consistent_shape, 1)\n",
        "    batch_size = 32 # You can adjust this batch size\n",
        "\n",
        "    train_generator = SpectrogramDataGenerator(all_training_files, batch_size=batch_size, shuffle=True, input_shape=generator_input_shape)\n",
        "    test_generator = SpectrogramDataGenerator(all_testing_files, batch_size=batch_size, shuffle=False, input_shape=generator_input_shape)\n",
        "\n",
        "    print(f\"Train generator created with 2D input shape {generator_input_shape} and {len(train_generator)} batches.\")\n",
        "    print(f\"Test generator created with 2D input shape {generator_input_shape} and {len(test_generator)} batches.\")\n",
        "else:\n",
        "    train_generator = None\n",
        "    test_generator = None\n",
        "    print(\"Cannot create data generators without a valid consistent shape.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determining spectrogram shape...\n",
            "Inferred spectrogram shape: (1024, 292)\n",
            "Using consistent shape: (1024, 292)\n",
            "\n",
            "Creating data generator instances with 2D input shape and correct paths...\n",
            "Train generator created with 2D input shape (1024, 292, 1) and 113 batches.\n",
            "Test generator created with 2D input shape (1024, 292, 1) and 22 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf1cfe7"
      },
      "source": [
        "### Define and build the Conv2D autoencoder model\n",
        "\n",
        "Define the architecture of a 2D convolutional autoencoder using Conv2D, MaxPooling2D, UpSampling2D, and Cropping2D layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "576dd044",
        "outputId": "f1217a1e-2ed2-4c91-ccf8-f712bcf08f9f"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define and build the 2D convolutional autoencoder model with shape adjustments\n",
        "print(\"\\nDefining and building the 2D convolutional autoencoder model...\")\n",
        "\n",
        "# Use the generator_input_shape determined in the previous step\n",
        "# It should be in the format (height, width, channels)\n",
        "if 'generator_input_shape' in locals() and generator_input_shape is not None:\n",
        "    input_shape = generator_input_shape\n",
        "    print(f\"Model input shape: {input_shape}\")\n",
        "\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "\n",
        "    # Add Cropping2D to match the target width (adjust cropping based on actual shape)\n",
        "    # Assuming the original shape was (1024, 292) and after 3x MaxPooling(2,2) and 3x UpSampling(2,2)\n",
        "    # the shape becomes (1024, 296). We need to crop the width from 296 to 292.\n",
        "    # This cropping might need adjustment if the original shape or pooling/upsampling changes.\n",
        "    # Based on previous output, the shape after upsampling was (1024, 296).\n",
        "    # Need to crop 2 from each side of the width: (0,0) for height, (2,2) for width\n",
        "    x = Cropping2D(cropping=((0, 0), (2, 2)))(x)\n",
        "\n",
        "    # Final Conv2D layer to get to 1 channel\n",
        "    decoded = Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
        "\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "    # Compile the autoencoder\n",
        "    print(\"\\nCompiling the autoencoder model...\")\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nAutoencoder model summary:\")\n",
        "    autoencoder.summary()\n",
        "else:\n",
        "    print(\"Input shape not determined. Cannot define Conv2D autoencoder model.\")\n",
        "    autoencoder = None"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining and building the 2D convolutional autoencoder model...\n",
            "Model input shape: (1024, 292, 1)\n",
            "\n",
            "Compiling the autoencoder model...\n",
            "\n",
            "Autoencoder model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m292\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m292\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_3 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m73,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_4 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m18,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_5 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m296\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cropping2d_1 (\u001b[38;5;33mCropping2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m292\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m292\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │           \u001b[38;5;34m289\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ up_sampling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ cropping2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m332,801\u001b[0m (1.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">332,801</span> (1.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m332,801\u001b[0m (1.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">332,801</span> (1.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de5a941"
      },
      "source": [
        "### Train the Conv2D autoencoder\n",
        "\n",
        "Train the defined Conv2D autoencoder model using the training data generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1171e983",
        "outputId": "6adf877b-d84f-423f-dfb1-bdc514cfce9b"
      },
      "source": [
        "# Train the autoencoder using the generator\n",
        "if autoencoder is not None and train_generator is not None:\n",
        "    print(\"\\nTraining the autoencoder with the data generator...\")\n",
        "    # Use .fit() with the generator\n",
        "    history = autoencoder.fit(train_generator,\n",
        "                              epochs=10, # Number of epochs\n",
        "                              # steps_per_epoch is automatically inferred from the generator's __len__()\n",
        "                              # validation_data can be a generator\n",
        "                              validation_data=test_generator, # Use test generator for validation during training\n",
        "                              shuffle=False # Shuffling is handled within the generator\n",
        "                             )\n",
        "    print(\"Autoencoder training with data generator complete.\")\n",
        "else:\n",
        "    print(\"\\nAutoencoder model or train/test generator not available. Skipping training.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the autoencoder with the data generator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 632ms/step - loss: 1370.3004 - val_loss: 33.4864\n",
            "Epoch 2/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 606ms/step - loss: 31.6034 - val_loss: 30.7766\n",
            "Epoch 3/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 590ms/step - loss: 30.6756 - val_loss: 30.7789\n",
            "Epoch 4/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 610ms/step - loss: 30.6092 - val_loss: 30.5959\n",
            "Epoch 5/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 599ms/step - loss: 30.6180 - val_loss: 30.5401\n",
            "Epoch 6/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 610ms/step - loss: 30.4690 - val_loss: 30.5917\n",
            "Epoch 7/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 588ms/step - loss: 30.6249 - val_loss: 30.4611\n",
            "Epoch 8/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 613ms/step - loss: 30.5446 - val_loss: 30.3398\n",
            "Epoch 9/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 591ms/step - loss: 30.4244 - val_loss: 30.4163\n",
            "Epoch 10/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 615ms/step - loss: 30.3197 - val_loss: 30.2213\n",
            "Autoencoder training with data generator complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e884fcd"
      },
      "source": [
        "### Evaluate the Conv2D autoencoder\n",
        "\n",
        "Evaluate the trained Conv2D autoencoder using the testing data generator and analyze reconstruction errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "1c59cd17",
        "outputId": "7bc257f6-b0a4-44c0-d748-667b7f415e40"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the autoencoder using the test generator\n",
        "if autoencoder is not None and test_generator is not None:\n",
        "    print(\"\\nEvaluating the autoencoder with the data generator...\")\n",
        "\n",
        "    # Predict using the test generator\n",
        "    reconstructions = autoencoder.predict(test_generator)\n",
        "\n",
        "    # Get original test data from the generator for MSE calculation\n",
        "    # Note: This will load the test data again in batches.\n",
        "    # A more memory-efficient approach for evaluation might be needed for very large test sets.\n",
        "    # For simplicity here, we will iterate through the test generator to get original data\n",
        "    original_test_data = []\n",
        "    for i in range(len(test_generator)):\n",
        "        batch_data, _ = test_generator[i]\n",
        "        original_test_data.append(batch_data)\n",
        "\n",
        "    if original_test_data:\n",
        "        original_test_data = np.concatenate(original_test_data, axis=0)\n",
        "        print(f\"Shape of original test data from generator: {original_test_data.shape}\")\n",
        "        print(f\"Shape of reconstructions: {reconstructions.shape}\")\n",
        "\n",
        "        # Ensure shapes match before calculating MSE\n",
        "        if original_test_data.shape == reconstructions.shape:\n",
        "            # Calculate MSE for each sample (across height, width, and channel dimensions)\n",
        "            mse = np.mean(np.power(original_test_data - reconstructions, 2), axis=(1, 2, 3))\n",
        "            print(f\"Shape of reconstruction errors (MSE per sample): {mse.shape}\")\n",
        "\n",
        "            # Plot the distribution of reconstruction errors\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.hist(mse, bins=50, density=True, alpha=0.7, color='skyblue')\n",
        "            plt.title('Distribution of Reconstruction Errors (MSE) on Testing Data (from Generator)')\n",
        "            plt.xlabel('Reconstruction Error (MSE)')\n",
        "            plt.ylabel('Density')\n",
        "            plt.grid(True)\n",
        "            display(plt) # Explicitly display the plot\n",
        "            plt.close() # Close the plot to free up memory\n",
        "\n",
        "            # Print statistics about the reconstruction errors\n",
        "            print(f\"Mean Reconstruction Error (MSE): {np.mean(mse)}\")\n",
        "            print(f\"Median Reconstruction Error (MSE): {np.median(mse)}\")\n",
        "            print(f\"Standard Deviation of Reconstruction Error (MSE): {np.std(mse)}\")\n",
        "            print(f\"Maximum Reconstruction Error (MSE): {np.max(mse)}\")\n",
        "            print(f\"Minimum Reconstruction Error (MSE): {np.min(mse)}\")\n",
        "        else:\n",
        "            print(\"Shape mismatch between original test data and reconstructions. Cannot calculate MSE.\")\n",
        "            print(f\"Original test data shape: {original_test_data.shape}\")\n",
        "            print(f\"Reconstructions shape: {reconstructions.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No original test data loaded from generator.\")\n",
        "\n",
        "else:\n",
        "     print(\"\\nAutoencoder model or test generator not available. Skipping evaluation.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the autoencoder with the data generator...\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 199ms/step\n",
            "Shape of original test data from generator: (704, 1024, 292, 1)\n",
            "Shape of reconstructions: (704, 1024, 292, 1)\n",
            "Shape of reconstruction errors (MSE per sample): (704,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reconstruction Error (MSE): 30.221345901489258\n",
            "Median Reconstruction Error (MSE): 30.622276306152344\n",
            "Standard Deviation of Reconstruction Error (MSE): 1.8326497077941895\n",
            "Maximum Reconstruction Error (MSE): 40.88224411010742\n",
            "Minimum Reconstruction Error (MSE): 26.259309768676758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa3f3697"
      },
      "source": [
        "### Visualize Reconstructions\n",
        "\n",
        "Visualize some original and reconstructed spectrograms from the testing data generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "6c9280c7",
        "outputId": "e4e19d34-49f4-46c7-cdd8-909cb1e28970"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming test_generator and autoencoder are available\n",
        "\n",
        "if autoencoder is not None and test_generator is not None and len(test_generator) > 0:\n",
        "    print(\"\\nVisualizing reconstructions...\")\n",
        "    # Get a batch of test data from the generator\n",
        "    original_batch, _ = test_generator[0] # Get the first batch\n",
        "\n",
        "    # Get reconstructions for this batch\n",
        "    reconstructed_batch = autoencoder.predict(original_batch)\n",
        "\n",
        "    # Select a few examples from the batch to visualize\n",
        "    num_examples = min(5, original_batch.shape[0]) # Visualize up to 5 examples or fewer if batch is smaller\n",
        "    example_indices = np.random.choice(original_batch.shape[0], num_examples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 6 * num_examples))\n",
        "\n",
        "    for i, idx in enumerate(example_indices):\n",
        "        # Original spectrogram (remove batch and channel dimensions for plotting)\n",
        "        original_spectrogram = np.squeeze(original_batch[idx])\n",
        "\n",
        "        # Reconstructed spectrogram (remove batch and channel dimensions for plotting)\n",
        "        reconstructed_spectrogram = np.squeeze(reconstructed_batch[idx])\n",
        "\n",
        "        # Plot original\n",
        "        plt.subplot(num_examples, 2, 2 * i + 1)\n",
        "        plt.imshow(original_spectrogram, aspect='auto', origin='lower', cmap='viridis') # Use imshow for 2D data\n",
        "        plt.title(f'Original Example {idx+1}')\n",
        "        plt.xlabel('Time') # Assuming x-axis is time\n",
        "        plt.ylabel('Frequency') # Assuming y-axis is frequency\n",
        "\n",
        "\n",
        "        # Plot reconstruction\n",
        "        plt.subplot(num_examples, 2, 2 * i + 2)\n",
        "        plt.imshow(reconstructed_spectrogram, aspect='auto', origin='lower', cmap='viridis') # Use imshow for 2D data\n",
        "        plt.title(f'Reconstruction Example {idx+1}')\n",
        "        plt.xlabel('Time') # Assuming x-axis is time\n",
        "        plt.ylabel('Frequency') # Assuming y-axis is frequency\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    display(plt) # Explicitly display the plot\n",
        "    plt.close() # Close the plot to free up memory\n",
        "\n",
        "else:\n",
        "    print(\"\\nTest generator or autoencoder not available, or test generator is empty. Skipping visualization.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing reconstructions...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b80e733"
      },
      "source": [
        "### Identify Potential Anomalies based on Reconstruction Error\n",
        "\n",
        "Use the calculated reconstruction errors to identify potential anomalies in the testing data based on a threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560fe23c",
        "outputId": "e14a7e8e-ec4b-46b7-a21f-048f6f0b6540"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming mse (reconstruction errors) is available from the evaluation cell (1c59cd17)\n",
        "# Assuming test_generator is available from previous steps\n",
        "\n",
        "if 'mse' in locals() and test_generator is not None:\n",
        "    print(\"Identifying potential anomalies...\")\n",
        "\n",
        "    # Calculate a simple threshold based on the mean and standard deviation of the MSE\n",
        "    mean_mse = np.mean(mse)\n",
        "    std_mse = np.std(mse)\n",
        "    # A common threshold is Mean + a multiple of StdDev (e.g., 2 or 3)\n",
        "    # You can adjust the multiplier based on how sensitive you want the anomaly detection to be\n",
        "    anomaly_threshold = mean_mse + 2 * std_mse\n",
        "\n",
        "    print(f\"\\nCalculated Anomaly Threshold (Mean + 2*StdDev): {anomaly_threshold}\")\n",
        "\n",
        "    # Identify samples in the testing data with a reconstruction error above the threshold\n",
        "    potential_anomaly_indices = np.where(mse > anomaly_threshold)[0]\n",
        "\n",
        "    print(f\"\\nNumber of potential anomalies detected in the testing data: {len(potential_anomaly_indices)}\")\n",
        "\n",
        "    # To find which frequency centers are involved, we would need to relate these indices\n",
        "    # back to the original file names and the frequency information within those files.\n",
        "    # The current data generator loads batches, so tracing back the index in the concatenated\n",
        "    # array to the original file and its content requires additional logic in the generator\n",
        "    # or during data loading to keep track of the origin of each sample.\n",
        "\n",
        "    # For now, let's list the indices of the potential anomalies\n",
        "    if len(potential_anomaly_indices) > 0:\n",
        "        print(\"Indices of potential anomalies (in the concatenated testing data array):\")\n",
        "        # Print only the first few indices if there are many\n",
        "        print(potential_anomaly_indices[:10])\n",
        "        if len(potential_anomaly_indices) > 10:\n",
        "            print(\"...\")\n",
        "\n",
        "    else:\n",
        "        print(\"No potential anomalies detected above the calculated threshold.\")\n",
        "\n",
        "    # Note: Relating indices back to specific frequency centers directly from the concatenated\n",
        "    # MSE array is not straightforward without mapping the indices back to the original\n",
        "    # spectrogram structure and associated metadata (like frequency centers if available).\n",
        "    # This would require modifications to the data loading/generator process.\n",
        "\n",
        "else:\n",
        "    print(\"Reconstruction errors (mse) or test generator not available. Cannot identify anomalies.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identifying potential anomalies...\n",
            "\n",
            "Calculated Anomaly Threshold (Mean + 2*StdDev): 33.88664627075195\n",
            "\n",
            "Number of potential anomalies detected in the testing data: 21\n",
            "Indices of potential anomalies (in the concatenated testing data array):\n",
            "[ 18 162 167 190 222 231 272 276 303 341]\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWtlFMxHWH8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1w8r2fiWIJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAk4bi4dWITs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmo_hPwMWIdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxdhUuRzWImZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff24acf8"
      },
      "source": [
        "# Task\n",
        "Modify the code to identify which files in the \"spectrograms_anomalous\" folder contain the detected anomalies and list those file names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd73eb4"
      },
      "source": [
        "## Modify testing data loading\n",
        "\n",
        "### Subtask:\n",
        "Modify the testing data loading process within the `SpectrogramDataGenerator` to store the original file path along with the loaded data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d580118b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to modify the data generator to return file paths and then update the evaluation loop to capture these paths. I will start by modifying the `SpectrogramDataGenerator` class to include the file paths in the `__getitem__` method's return value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86f63d8",
        "outputId": "f27d6af8-a6d1-4347-a96b-1dd57f27923c"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class SpectrogramDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Data Generator for loading 2D spectrograms from .npy files in batches for Conv2D input.\n",
        "    Includes functionality to return corresponding file paths.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_paths, batch_size=32, shuffle=True, input_shape=(1024, 292, 1)): # Expected Conv2D input shape\n",
        "        self.file_paths = file_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        # Input shape should now include the channel dimension for Conv2D\n",
        "        self.input_shape = input_shape\n",
        "        self.indexes = np.arange(len(self.file_paths))\n",
        "        if self.shuffle:\n",
        "            self.on_epoch_end()\n",
        "        # Determine the expected 2D shape without the channel for loading\n",
        "        self._loading_shape = input_shape[:2]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        # Ensure we don't have empty batches at the end if drop_last is not used\n",
        "        return int(np.floor(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data and corresponding file paths\"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # List of file paths for the batch\n",
        "        batch_file_paths = [self.file_paths[k] for k in indexes]\n",
        "\n",
        "        # Load and collect data for the batch\n",
        "        batch_data = []\n",
        "        loaded_batch_file_paths = [] # Store paths for successfully loaded data\n",
        "        for file_path in batch_file_paths:\n",
        "            try:\n",
        "                data = np.load(file_path, mmap_mode='r')\n",
        "                # Ensure data matches the expected loading shape (without channel)\n",
        "                if data.shape == self._loading_shape:\n",
        "                    # Add channel dimension (grayscale)\n",
        "                    batch_data.append(np.expand_dims(data, axis=-1))\n",
        "                    loaded_batch_file_paths.append(file_path)\n",
        "                else:\n",
        "                    print(f\"Warning: Spectrogram shape mismatch for {file_path}. Expected {self._loading_shape}, got {data.shape}. Skipping.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading file {file_path} in generator: {e}\")\n",
        "\n",
        "        if not batch_data:\n",
        "             # If no data was loaded for this batch, return empty arrays and an empty list of paths\n",
        "             return np.empty((0, *self.input_shape)), np.empty((0, *self.input_shape)), []\n",
        "\n",
        "        # Concatenate data from all files in the batch\n",
        "        try:\n",
        "             batch_data = np.stack(batch_data, axis=0) # Stack along the new batch dimension\n",
        "        except ValueError as e:\n",
        "             print(f\"Error stacking data for batch index {index}: {e}\")\n",
        "             # Returning empty arrays and an empty list on error\n",
        "             return np.empty((0, *self.input_shape)), np.empty((0, *self.input_shape)), []\n",
        "\n",
        "\n",
        "        # Assuming the autoencoder input and output are the same\n",
        "        # Return batch_data twice (for input and target) and the list of file paths\n",
        "        return batch_data, batch_data, loaded_batch_file_paths\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "print(\"SpectrogramDataGenerator class modified to return file paths.\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpectrogramDataGenerator class modified to return file paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5337557b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `SpectrogramDataGenerator` is modified to return file paths, I need to update the evaluation loop in the next code block to capture these file paths when iterating through the test generator and store them in a list that corresponds to the order of the concatenated data and reconstruction arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a689d191",
        "outputId": "f386d7d2-f032-4044-ba71-16bc4351fe7b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the autoencoder using the test generator\n",
        "if autoencoder is not None and test_generator is not None:\n",
        "    print(\"\\nEvaluating the autoencoder with the data generator...\")\n",
        "\n",
        "    # Predict using the test generator\n",
        "    # The predict method of the model does not directly give access to the generator's file paths\n",
        "    # We will iterate through the generator to get original data and file paths,\n",
        "    # and then predict on those batches.\n",
        "\n",
        "    original_test_data = []\n",
        "    all_test_file_paths = [] # List to store all file paths from the test generator\n",
        "    reconstructions_list = [] # List to store reconstructions for each batch\n",
        "\n",
        "    print(\"Loading test data and getting reconstructions batch by batch...\")\n",
        "    for i in range(len(test_generator)):\n",
        "        try:\n",
        "            # Get a batch of original data and file paths from the generator\n",
        "            batch_data, _, batch_file_paths = test_generator[i]\n",
        "\n",
        "            if batch_data.shape[0] > 0: # Check if the batch is not empty\n",
        "                # Predict reconstructions for this batch\n",
        "                batch_reconstructions = autoencoder.predict(batch_data, verbose=0) # verbose=0 to reduce output\n",
        "\n",
        "                # Append batch data, reconstructions, and file paths to the lists\n",
        "                original_test_data.append(batch_data)\n",
        "                reconstructions_list.append(batch_reconstructions)\n",
        "                all_test_file_paths.extend(batch_file_paths) # Extend with the list of file paths\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {i}: {e}\")\n",
        "            # Continue to the next batch if there's an error\n",
        "\n",
        "\n",
        "    if original_test_data and reconstructions_list:\n",
        "        # Concatenate all batches to get the full arrays\n",
        "        original_test_data = np.concatenate(original_test_data, axis=0)\n",
        "        reconstructions = np.concatenate(reconstructions_list, axis=0)\n",
        "\n",
        "        print(f\"Shape of original test data from generator: {original_test_data.shape}\")\n",
        "        print(f\"Shape of reconstructions: {reconstructions.shape}\")\n",
        "        print(f\"Number of test file paths collected: {len(all_test_file_paths)}\")\n",
        "\n",
        "\n",
        "        # Ensure shapes match before calculating MSE and file paths match the data length\n",
        "        if original_test_data.shape == reconstructions.shape and len(all_test_file_paths) == original_test_data.shape[0]:\n",
        "            # Calculate MSE for each sample (across height, width, and channel dimensions)\n",
        "            mse = np.mean(np.power(original_test_data - reconstructions, 2), axis=(1, 2, 3))\n",
        "            print(f\"Shape of reconstruction errors (MSE per sample): {mse.shape}\")\n",
        "\n",
        "            # Plot the distribution of reconstruction errors\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.hist(mse, bins=50, density=True, alpha=0.7, color='skyblue')\n",
        "            plt.title('Distribution of Reconstruction Errors (MSE) on Testing Data')\n",
        "            plt.xlabel('Reconstruction Error (MSE)')\n",
        "            plt.ylabel('Density')\n",
        "            plt.grid(True)\n",
        "            display(plt) # Explicitly display the plot\n",
        "            plt.close() # Close the plot to free up memory\n",
        "\n",
        "            # Print statistics about the reconstruction errors\n",
        "            print(f\"Mean Reconstruction Error (MSE): {np.mean(mse)}\")\n",
        "            print(f\"Median Reconstruction Error (MSE): {np.median(mse)}\")\n",
        "            print(f\"Standard Deviation of Reconstruction Error (MSE): {np.std(mse)}\")\n",
        "            print(f\"Maximum Reconstruction Error (MSE): {np.max(mse)}\")\n",
        "            print(f\"Minimum Reconstruction Error (MSE): {np.min(mse)}\")\n",
        "        else:\n",
        "            print(\"Shape mismatch between original test data, reconstructions, or file paths count. Cannot calculate MSE or proceed.\")\n",
        "            print(f\"Original test data shape: {original_test_data.shape}\")\n",
        "            print(f\"Reconstructions shape: {reconstructions.shape}\")\n",
        "            print(f\"Number of test file paths collected: {len(all_test_file_paths)}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No data loaded from test generator for evaluation.\")\n",
        "\n",
        "\n",
        "else:\n",
        "     print(\"\\nAutoencoder model or test generator not available. Skipping evaluation.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the autoencoder with the data generator...\n",
            "Loading test data and getting reconstructions batch by batch...\n",
            "Error processing batch 0: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 1: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 2: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 3: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 4: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 5: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 6: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 7: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 8: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 9: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 10: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 11: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 12: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 13: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 14: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 15: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 16: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 17: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 18: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 19: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 20: not enough values to unpack (expected 3, got 2)\n",
            "Error processing batch 21: not enough values to unpack (expected 3, got 2)\n",
            "No data loaded from test generator for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29756974"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message \"not enough values to unpack (expected 3, got 2)\" indicates that the `test_generator[i]` is still returning 2 values instead of the expected 3 (batch data, targets, file paths). This is because the `SpectrogramDataGenerator` was redefined in a previous cell, but the `test_generator` instance itself was not recreated after the class definition was updated. I need to recreate the `test_generator` instance using the updated class definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b136dc3",
        "outputId": "30a5a573-75b7-4680-8dbd-3ba81a3e1806"
      },
      "source": [
        "# Recreate the data generator instances with the updated class definition\n",
        "print(\"\\nRecreating data generator instances with the updated SpectrogramDataGenerator class...\")\n",
        "\n",
        "if consistent_shape is not None:\n",
        "    # Input shape for the generator and model, including the channel dimension\n",
        "    # Use variables defined in previous steps\n",
        "    # all_training_files, all_testing_files, batch_size, generator_input_shape\n",
        "    train_generator = SpectrogramDataGenerator(all_training_files, batch_size=batch_size, shuffle=True, input_shape=generator_input_shape)\n",
        "    test_generator = SpectrogramDataGenerator(all_testing_files, batch_size=batch_size, shuffle=False, input_shape=generator_input_shape)\n",
        "\n",
        "    print(f\"Train generator recreated with 2D input shape {generator_input_shape} and {len(train_generator)} batches.\")\n",
        "    print(f\"Test generator recreated with 2D input shape {generator_input_shape} and {len(test_generator)} batches.\")\n",
        "else:\n",
        "    train_generator = None\n",
        "    test_generator = None\n",
        "    print(\"Consistent shape not determined. Cannot recreate data generators.\")\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recreating data generator instances with the updated SpectrogramDataGenerator class...\n",
            "Train generator recreated with 2D input shape (1024, 292, 1) and 113 batches.\n",
            "Test generator recreated with 2D input shape (1024, 292, 1) and 22 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20e24d39"
      },
      "source": [
        "**Reasoning**:\n",
        "The `test_generator` has been successfully recreated with the updated class definition. Now I can re-run the evaluation code block which is expected to correctly unpack the 3 returned values and proceed with the evaluation, capturing the file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "9bfca8c9",
        "outputId": "3366625b-61bc-4f22-8141-c9c5530e4c1d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the autoencoder using the test generator\n",
        "if autoencoder is not None and test_generator is not None:\n",
        "    print(\"\\nEvaluating the autoencoder with the data generator...\")\n",
        "\n",
        "    # Predict using the test generator\n",
        "    # The predict method of the model does not directly give access to the generator's file paths\n",
        "    # We will iterate through the generator to get original data and file paths,\n",
        "    # and then predict on those batches.\n",
        "\n",
        "    original_test_data = []\n",
        "    all_test_file_paths = [] # List to store all file paths from the test generator\n",
        "    reconstructions_list = [] # List to store reconstructions for each batch\n",
        "\n",
        "    print(\"Loading test data and getting reconstructions batch by batch...\")\n",
        "    for i in range(len(test_generator)):\n",
        "        try:\n",
        "            # Get a batch of original data and file paths from the generator\n",
        "            batch_data, _, batch_file_paths = test_generator[i]\n",
        "\n",
        "            if batch_data.shape[0] > 0: # Check if the batch is not empty\n",
        "                # Predict reconstructions for this batch\n",
        "                batch_reconstructions = autoencoder.predict(batch_data, verbose=0) # verbose=0 to reduce output\n",
        "\n",
        "                # Append batch data, reconstructions, and file paths to the lists\n",
        "                original_test_data.append(batch_data)\n",
        "                reconstructions_list.append(batch_reconstructions)\n",
        "                all_test_file_paths.extend(batch_file_paths) # Extend with the list of file paths\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {i}: {e}\")\n",
        "            # Continue to the next batch if there's an error\n",
        "\n",
        "\n",
        "    if original_test_data and reconstructions_list:\n",
        "        # Concatenate all batches to get the full arrays\n",
        "        original_test_data = np.concatenate(original_test_data, axis=0)\n",
        "        reconstructions = np.concatenate(reconstructions_list, axis=0)\n",
        "\n",
        "        print(f\"Shape of original test data from generator: {original_test_data.shape}\")\n",
        "        print(f\"Shape of reconstructions: {reconstructions.shape}\")\n",
        "        print(f\"Number of test file paths collected: {len(all_test_file_paths)}\")\n",
        "\n",
        "\n",
        "        # Ensure shapes match before calculating MSE and file paths match the data length\n",
        "        if original_test_data.shape == reconstructions.shape and len(all_test_file_paths) == original_test_data.shape[0]:\n",
        "            # Calculate MSE for each sample (across height, width, and channel dimensions)\n",
        "            mse = np.mean(np.power(original_test_data - reconstructions, 2), axis=(1, 2, 3))\n",
        "            print(f\"Shape of reconstruction errors (MSE per sample): {mse.shape}\")\n",
        "\n",
        "            # Plot the distribution of reconstruction errors\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.hist(mse, bins=50, density=True, alpha=0.7, color='skyblue')\n",
        "            plt.title('Distribution of Reconstruction Errors (MSE) on Testing Data')\n",
        "            plt.xlabel('Reconstruction Error (MSE)')\n",
        "            plt.ylabel('Density')\n",
        "            plt.grid(True)\n",
        "            display(plt) # Explicitly display the plot\n",
        "            plt.close() # Close the plot to free up memory\n",
        "\n",
        "            # Print statistics about the reconstruction errors\n",
        "            print(f\"Mean Reconstruction Error (MSE): {np.mean(mse)}\")\n",
        "            print(f\"Median Reconstruction Error (MSE): {np.median(mse)}\")\n",
        "            print(f\"Standard Deviation of Reconstruction Error (MSE): {np.std(mse)}\")\n",
        "            print(f\"Maximum Reconstruction Error (MSE): {np.max(mse)}\")\n",
        "            print(f\"Minimum Reconstruction Error (MSE): {np.min(mse)}\")\n",
        "        else:\n",
        "            print(\"Shape mismatch between original test data, reconstructions, or file paths count. Cannot calculate MSE or proceed.\")\n",
        "            print(f\"Original test data shape: {original_test_data.shape}\")\n",
        "            print(f\"Reconstructions shape: {reconstructions.shape}\")\n",
        "            print(f\"Number of test file paths collected: {len(all_test_file_paths)}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No data loaded from test generator for evaluation.\")\n",
        "\n",
        "\n",
        "else:\n",
        "     print(\"\\nAutoencoder model or test generator not available. Skipping evaluation.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the autoencoder with the data generator...\n",
            "Loading test data and getting reconstructions batch by batch...\n",
            "Shape of original test data from generator: (704, 1024, 292, 1)\n",
            "Shape of reconstructions: (704, 1024, 292, 1)\n",
            "Number of test file paths collected: 704\n",
            "Shape of reconstruction errors (MSE per sample): (704,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reconstruction Error (MSE): 30.221345901489258\n",
            "Median Reconstruction Error (MSE): 30.622276306152344\n",
            "Standard Deviation of Reconstruction Error (MSE): 1.8326497077941895\n",
            "Maximum Reconstruction Error (MSE): 40.88224411010742\n",
            "Minimum Reconstruction Error (MSE): 26.259309768676758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43124b15"
      },
      "source": [
        "## Create an index-to-file mapping\n",
        "\n",
        "### Subtask:\n",
        "Create a mapping (e.g., a list or array) that stores the original file path for each sample in the concatenated testing data array (`original_test_data`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cac8468"
      },
      "source": [
        "## Update anomaly identification\n",
        "\n",
        "### Subtask:\n",
        "Update the anomaly identification code to use the calculated anomaly indices and the `all_test_file_paths` list to determine which original files contain the potentially anomalous samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e895075c"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the indices of potential anomalies based on the calculated threshold and retrieve the corresponding file paths from the all_test_file_paths list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63898ff",
        "outputId": "f998bd47-aa33-4085-bb8f-e640722fc253"
      },
      "source": [
        "# Assuming mse (reconstruction errors) is available from the evaluation cell (1c59cd17)\n",
        "# Assuming all_test_file_paths is available from the evaluation cell (1c59cd17)\n",
        "\n",
        "if 'mse' in locals() and 'all_test_file_paths' in locals() and all_test_file_paths:\n",
        "    print(\"Identifying potential anomalies and their corresponding files...\")\n",
        "\n",
        "    # Calculate a simple threshold based on the mean and standard deviation of the MSE\n",
        "    mean_mse = np.mean(mse)\n",
        "    std_mse = np.std(mse)\n",
        "    anomaly_threshold = mean_mse + 2 * std_mse # Using Mean + 2*StdDev as threshold\n",
        "\n",
        "    print(f\"\\nCalculated Anomaly Threshold (Mean + 2*StdDev): {anomaly_threshold}\")\n",
        "\n",
        "    # Identify samples in the testing data with a reconstruction error above the threshold\n",
        "    potential_anomaly_indices = np.where(mse > anomaly_threshold)[0]\n",
        "\n",
        "    # Retrieve the corresponding file paths using the indices\n",
        "    anomalous_file_paths = [all_test_file_paths[i] for i in potential_anomaly_indices]\n",
        "\n",
        "    print(f\"\\nNumber of potential anomalies detected in the testing data: {len(potential_anomaly_indices)}\")\n",
        "\n",
        "    if len(anomalous_file_paths) > 0:\n",
        "        print(\"File paths of potential anomalies:\")\n",
        "        # Print the first few file paths if there are many\n",
        "        for i, file_path in enumerate(anomalous_file_paths[:10]):\n",
        "            print(f\"- {file_path}\")\n",
        "        if len(anomalous_file_paths) > 10:\n",
        "            print(\"...\")\n",
        "    else:\n",
        "        print(\"No potential anomalies detected above the calculated threshold.\")\n",
        "\n",
        "else:\n",
        "    print(\"Reconstruction errors (mse) or all_test_file_paths not available. Cannot identify anomaly files.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identifying potential anomalies and their corresponding files...\n",
            "\n",
            "Calculated Anomaly Threshold (Mean + 2*StdDev): 33.88664627075195\n",
            "\n",
            "Number of potential anomalies detected in the testing data: 21\n",
            "File paths of potential anomalies:\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_773MHz_20251025_082303.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_103MHz_20251025_082148.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_792MHz_20251025_082304.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_800MHz_20251025_082305.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_934MHz_20251025_082316.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_153MHz_20251025_082155.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_783MHz_20251025_082304.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_93MHz_20251025_082145.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_802MHz_20251025_082305.npy\n",
            "- /tmp/extracted_testing_data/spectrograms_anomolous/spec_809MHz_20251025_082306.npy\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01c5d4ce"
      },
      "source": [
        "## List anomalous files\n",
        "\n",
        "### Subtask:\n",
        "Present a list of the file names from the `spectrograms_anomolous` folder that contain samples identified as potential anomalies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21d66e18"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the base file names from the anomalous file paths and print them with a clear heading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f0ac491",
        "outputId": "53f4e31c-81b2-4930-971f-ff3e68bee6cf"
      },
      "source": [
        "import os\n",
        "\n",
        "if 'anomalous_file_paths' in locals() and anomalous_file_paths:\n",
        "    # Extract the base file names\n",
        "    anomalous_base_file_names = [os.path.basename(f) for f in anomalous_file_paths]\n",
        "\n",
        "    print(\"\\nPotential Anomaly Files:\")\n",
        "    for file_name in anomalous_base_file_names:\n",
        "        print(file_name)\n",
        "else:\n",
        "    print(\"No anomalous file paths found to list.\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Potential Anomaly Files:\n",
            "spec_773MHz_20251025_082303.npy\n",
            "spec_103MHz_20251025_082148.npy\n",
            "spec_792MHz_20251025_082304.npy\n",
            "spec_800MHz_20251025_082305.npy\n",
            "spec_934MHz_20251025_082316.npy\n",
            "spec_153MHz_20251025_082155.npy\n",
            "spec_783MHz_20251025_082304.npy\n",
            "spec_93MHz_20251025_082145.npy\n",
            "spec_802MHz_20251025_082305.npy\n",
            "spec_809MHz_20251025_082306.npy\n",
            "spec_927MHz_20251025_082316.npy\n",
            "spec_88MHz_20251025_082142.npy\n",
            "spec_932MHz_20251025_082316.npy\n",
            "spec_96MHz_20251025_082146.npy\n",
            "spec_804MHz_20251025_082305.npy\n",
            "spec_941MHz_20251025_082317.npy\n",
            "spec_91MHz_20251025_082143.npy\n",
            "spec_929MHz_20251025_082316.npy\n",
            "spec_105MHz_20251025_082150.npy\n",
            "spec_434MHz_20251025_082230.npy\n",
            "spec_98MHz_20251025_082148.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc2732be"
      },
      "source": [
        "### Rank Anomalies by Reconstruction Error\n",
        "\n",
        "Rank the identified potential anomalies based on their reconstruction error (MSE) and list the top anomalies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc53c0e2",
        "outputId": "cb5e7db0-8380-480b-bd02-e5ea4b259328"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assuming mse (reconstruction errors) is available from evaluation (cell 1c59cd17)\n",
        "# Assuming potential_anomaly_indices is available from anomaly identification (cell 560fe23c)\n",
        "# Assuming all_test_file_paths is available from evaluation with file paths (cell a689d191 or 9bfca8c9)\n",
        "\n",
        "if 'mse' in locals() and 'potential_anomaly_indices' in locals() and 'all_test_file_paths' in locals():\n",
        "    print(\"Ranking potential anomalies by reconstruction error...\")\n",
        "\n",
        "    # Get the MSE values for the potential anomalies\n",
        "    anomalous_mse_values = mse[potential_anomaly_indices]\n",
        "\n",
        "    # Sort the anomaly indices based on their MSE values in descending order\n",
        "    # Get the indices that would sort the anomalous_mse_values in descending order\n",
        "    ranked_indices_in_anomalous_list = np.argsort(anomalous_mse_values)[::-1]\n",
        "\n",
        "    # Use these sorted indices to get the original indices of the anomalies, ranked\n",
        "    ranked_anomaly_indices = potential_anomaly_indices[ranked_indices_in_anomalous_list]\n",
        "\n",
        "    print(f\"\\nFound {len(ranked_anomaly_indices)} potential anomalies to rank.\")\n",
        "\n",
        "    if len(ranked_anomaly_indices) > 0:\n",
        "        print(\"\\nRanked Potential Anomalies (by MSE):\")\n",
        "        print(\"Rank | MSE | File Name\")\n",
        "        print(\"-----|--------------------|-----------\")\n",
        "        # Display the top anomalies (e.g., top 10)\n",
        "        num_anomalies_to_show = min(20, len(ranked_anomaly_indices)) # Show up to 20 or fewer if less detected\n",
        "\n",
        "        for rank, original_index in enumerate(ranked_anomaly_indices[:num_anomalies_to_show]):\n",
        "            corresponding_mse = mse[original_index]\n",
        "            corresponding_file = all_test_file_paths[original_index]\n",
        "            # Extract just the base file name for cleaner display\n",
        "            file_name = os.path.basename(corresponding_file)\n",
        "            print(f\"{rank + 1:<4} | {corresponding_mse:<18.6f} | {file_name}\")\n",
        "\n",
        "        if len(ranked_anomaly_indices) > num_anomalies_to_show:\n",
        "            print(\"...\")\n",
        "    else:\n",
        "        print(\"No potential anomalies were identified to rank.\")\n",
        "\n",
        "else:\n",
        "    print(\"Required variables (mse, potential_anomaly_indices, all_test_file_paths) not available. Cannot rank anomalies.\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking potential anomalies by reconstruction error...\n",
            "\n",
            "Found 21 potential anomalies to rank.\n",
            "\n",
            "Ranked Potential Anomalies (by MSE):\n",
            "Rank | MSE | File Name\n",
            "-----|--------------------|-----------\n",
            "1    | 40.882244          | spec_932MHz_20251025_082316.npy\n",
            "2    | 38.116680          | spec_929MHz_20251025_082316.npy\n",
            "3    | 37.925835          | spec_96MHz_20251025_082146.npy\n",
            "4    | 37.772583          | spec_88MHz_20251025_082142.npy\n",
            "5    | 37.692722          | spec_804MHz_20251025_082305.npy\n",
            "6    | 37.340153          | spec_941MHz_20251025_082317.npy\n",
            "7    | 36.620926          | spec_802MHz_20251025_082305.npy\n",
            "8    | 35.902058          | spec_105MHz_20251025_082150.npy\n",
            "9    | 35.669212          | spec_927MHz_20251025_082316.npy\n",
            "10   | 35.273869          | spec_91MHz_20251025_082143.npy\n",
            "11   | 35.176708          | spec_800MHz_20251025_082305.npy\n",
            "12   | 34.982578          | spec_809MHz_20251025_082306.npy\n",
            "13   | 34.649902          | spec_103MHz_20251025_082148.npy\n",
            "14   | 34.535919          | spec_93MHz_20251025_082145.npy\n",
            "15   | 34.458092          | spec_783MHz_20251025_082304.npy\n",
            "16   | 34.368259          | spec_434MHz_20251025_082230.npy\n",
            "17   | 34.361576          | spec_153MHz_20251025_082155.npy\n",
            "18   | 34.352470          | spec_98MHz_20251025_082148.npy\n",
            "19   | 34.069729          | spec_934MHz_20251025_082316.npy\n",
            "20   | 34.013580          | spec_773MHz_20251025_082303.npy\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1a7f71"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `SpectrogramDataGenerator` class was successfully modified to return the original file paths along with the batch data.\n",
        "*   The evaluation loop was updated to collect the original test data, reconstructions, and corresponding file paths for each sample.\n",
        "*   A total of 704 test file paths were successfully collected, matching the number of samples processed.\n",
        "*   An anomaly threshold was calculated based on the mean and standard deviation of the reconstruction errors (MSE).\n",
        "*   Using this threshold, 21 potential anomalies were identified in the testing data.\n",
        "*   The file paths corresponding to these 21 anomalies were successfully retrieved and listed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Further analysis could involve investigating the characteristics of the identified anomalous spectrograms to understand the types of anomalies the autoencoder is detecting.\n",
        "*   The anomaly threshold could be further refined or determined using alternative methods (e.g., percentile-based) to potentially improve the precision or recall of anomaly detection.\n"
      ]
    }
  ]
}